# MultiverseGym

## Overview

`MultiverseGym` is a custom [OpenAI Gym](https://github.com/openai/gym) environment designed for language generation tasks. It provides a standardized interface for reinforcement learning agents to interact with and learn from multiple language models simultaneously. It allows for batch processing, customizable reward policies, and seamless integration with popular RL algorithms.

![MultiverseGym Setup](https://raw.githubusercontent.com/umairhaider/gym-multiverse/main/docs/multiverse-gym-setup.png)

The environment is built on top of the [OpenAI Gym](https://github.com/openai/gym) framework, which allows for easy integration with existing reinforcement learning algorithms and tools.

The main components of the MultiverseGym environment are as follows:

- **State**: The state represents the current context or partial text generated by the language models. It is a sequence of tokens encoded using the GPT-2 tokenizer. The state serves as the input for generating the next token.
- **Action**: The action refers to the next token to be generated by the language models. In the MultiverseGym environment, the action space is discrete and corresponds to the vocabulary of the language models.
- **Observation**: The observation is similar to the state and represents the current context or partial text generated by the language models. It is also a sequence of tokens encoded using the GPT-2 tokenizer.
- **Reward**: The reward is a scalar value that indicates the quality or desirability of the generated text. In the MultiverseGym environment, the default reward policy rewards the agent for generating novel words. However, this reward policy can be customized based on the specific language generation task.
- **Episode**: An episode refers to a complete sequence of interactions between the agent and the environment. In the context of language generation, an episode typically corresponds to the generation of a complete sentence, paragraph, or document.
- **Terminal State**: A terminal state indicates the end of an episode. In the MultiverseGym environment, a terminal state can be reached when a maximum sequence length is reached or when a specific condition is met.
- **Action Space**: The action space defines the set of possible actions that the agent can take. In the MultiverseGym environment, the action space is discrete and corresponds to the vocabulary of the language models.
- **Observation Space**: The observation space defines the set of possible observations that the agent can receive. In the MultiverseGym environment, the observation space is similar to the state and represents the current context or partial text generated by the language models.

This repository contains the implementation of the MultiverseGym environment and an example agent (`PPOC_Agent`) that utilizes the environment for training and inference.

## Features

- Multi-model Language Generation: Train your RL agent using multiple GPT-2 language models simultaneously to generate high-quality text.
- OpenAI Gym Integration: Follows the Gym interface, making it compatible with existing RL algorithms, frameworks, and tools.
- Batch Processing: Efficiently process multiple actions, rewards, and observations in parallel for faster training and improved sample efficiency.
- Customizable Reward Policies: Implement custom reward policies tailored to your specific language generation task.
- State-of-the-Art Language Models: Utilizes pre-trained GPT-2 models for powerful text generation capabilities.
- Extensible and Flexible: Easily customize and extend the environment to fit your unique language generation requirements.
- Reinforcement learning agent implementation using Proximal Policy Optimization with Clipped Objective (PPOC) algorithm

## Installation

1. Clone the repository:

```shell
git clone https://github.com/umairhaider/gym-multiverse.git
cd gym-multiverse
```

## Usage

To use the MultiverseGym environment, follow these steps:

1. Import the necessary modules:
```python
from envs.multiverse import MultiverseGym
```

2. Create an instance of the MultiverseGym environment:
```python
env = MultiverseGym(model_names=['gpt2', 'gpt2-medium'], initial_prompt="Once upon a time", max_length=100, batch_size=1)
```

3. Initialize and configure your reinforcement learning agent:
```python
from agents.ppoc_agent import PPOC_Agent

agent = PPOC_Agent(env)
agent.load_model('trained_model.pth')
```

4. Use the agent to interact with the environment:
```python
state = env.reset()
done = False
total_reward = 0

while not done:
    action, _ = agent.select_action(state)
    next_state, reward, done, _ = env.step(action)
    total_reward += reward
    state = next_state

print(f"Total reward with loaded model: {total_reward}")
```

## Customization

### Reward Policy
The MultiverseGym environment allows you to define a custom reward policy by subclassing the `MultiverseGym` class and overriding the `reward_policy` method. By default, the environment uses a reward policy that rewards the agent for generating novel words. You can modify this method to implement your own reward policy based on the specific requirements of your task.

### Additional Language Models
To use additional language models in the environment, you can pass the model names as a list when initializing the `MultiverseGym` instance. For example:
```python
env = MultiverseGym(model_names=['gpt2', 'gpt2-medium', 'gpt2-large'])
```